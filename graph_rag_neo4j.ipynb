{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cbtPSDoaiKt"
      },
      "source": [
        "# Graph RAG with Neo4j Aura and LangChain\n",
        "\n",
        "This notebook shows a **end-to-end example** of how to build **Graph RAG** (Graph-based Retrieval-Augmented Generation) system using:\n",
        "\n",
        "- **Neo4j Aura** as a managed graph database\n",
        "- **LangChain** for LLM + graph tools\n",
        "- **Groq** (ChatGroq) as the LLM provider\n",
        "\n",
        "we use **DevOps / incident management** scenario:\n",
        "\n",
        "- Services (e.g. `checkout-service`, `payment-service`)\n",
        "- Incidents with severities (P1, P2, etc.)\n",
        "- On-call engineers who handled incidents\n",
        "\n",
        "You will learn:\n",
        "\n",
        "1. How to connect Colab to **Neo4j Aura**\n",
        "2. How to create **incident knowledge graph**\n",
        "3. How to use an LLM to **extract a graph** from unstructured incident text\n",
        "4. How to ask **natural language questions** and let the LLM generate **Cypher** to query Neo4j"
      ],
      "id": "9cbtPSDoaiKt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0WPqU0VaiKy"
      },
      "source": [
        "## 1. Install dependencies\n",
        "\n",
        "This cell installs the Python packages we need:\n",
        "\n",
        "- `langchain`, `langchain-community`, `langchain-experimental` – core LangChain + extra utilities\n",
        "- `langchain-groq` – LangChain integration for Groq's LLMs\n",
        "- `neo4j` – official Neo4j Python driver\n",
        "\n",
        "We use `!` to run a shell command from the notebook, and `--quiet` to keep the output clean."
      ],
      "id": "P0WPqU0VaiKy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_mTxAP0aiK0"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet \\\n",
        "  langchain langchain-community langchain-experimental langchain-groq neo4j"
      ],
      "id": "O_mTxAP0aiK0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SBhq1weaiK1"
      },
      "source": [
        "## 2. Configure Neo4j Aura connection\n",
        "\n",
        "In this section, we configure how the notebook connects to Neo4j Aura.\n",
        "\n",
        "You need three values from your **Neo4j Aura** instance:\n",
        "\n",
        "- `NEO4J_URI` – the connection URL (starts with `neo4j+s://...`)\n",
        "- `NEO4J_USERNAME` – usually `neo4j` by default\n",
        "- `NEO4J_PASSWORD` – the password you chose when creating the database\n",
        "\n",
        "For safety, you should **not commit real passwords** to GitHub.  \n",
        "When publishing, replace the real password with a placeholder (as we do here)."
      ],
      "id": "9SBhq1weaiK1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXTZgTgZaiK2"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# TODO: replace the placeholder values with your real Aura connection details\n",
        "NEO4J_URI = \"neo4j+s://6419bb8d.databases.neo4j.io\"\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"xxxxxxxxxxxxxx\"\n",
        "\n",
        "# Store these in environment variables so that libraries can read them easily\n",
        "os.environ[\"NEO4J_URI\"] = NEO4J_URI\n",
        "os.environ[\"NEO4J_USERNAME\"] = NEO4J_USERNAME\n",
        "os.environ[\"NEO4J_PASSWORD\"] = NEO4J_PASSWORD"
      ],
      "id": "DXTZgTgZaiK2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIGUPHi_aiK3"
      },
      "source": [
        "## 3. Configure Groq (LLM) API key\n",
        "\n",
        "We will use **Groq** as the LLM provider through `langchain-groq`.\n",
        "\n",
        "1. Create an account at Groq and generate an API key.\n",
        "2. Paste your key into the `GROQ_API_KEY` variable below (or use a Colab secret).\n",
        "\n",
        "Again, do **not** commit real keys to GitHub. Use environment variables or Colab secrets in real projects."
      ],
      "id": "KIGUPHi_aiK3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXIgqwXjaiK3"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: replace the placeholder with your real Groq API key\n",
        "GROQ_API_KEY = \"gsk_xxxxxxxxxxxxxx\"\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY"
      ],
      "id": "uXIgqwXjaiK3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRWruAQTaiK4"
      },
      "source": [
        "## 4. Initialize Neo4j client and LLM\n",
        "\n",
        "Here we:\n",
        "\n",
        "1. Create a `Neo4jGraph` object so LangChain can talk to Neo4j.\n",
        "2. Create a `ChatGroq` LLM instance.\n",
        "\n",
        "We will use:\n",
        "\n",
        "- `Neo4jGraph` to run Cypher and inspect the schema.\n",
        "- `ChatGroq` as the brain that:\n",
        "  - Helps extract graphs from text.\n",
        "  - Translates natural language questions into Cypher."
      ],
      "id": "YRWruAQTaiK4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE54xbjQaiK4",
        "outputId": "f98e6442-ef62-4343-ce21-4d77c0992541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<langchain_community.graphs.neo4j_graph.Neo4jGraph at 0x7e26355f2150>,\n",
              " ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7e263547ba10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7e262fe1f020>, model_name='llama-3.3-70b-versatile', temperature=1e-08, model_kwargs={}, groq_api_key=SecretStr('**********')))"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Initialize the Neo4j graph connection\n",
        "graph = Neo4jGraph(\n",
        "    url=NEO4J_URI,\n",
        "    username=NEO4J_USERNAME,\n",
        "    password=NEO4J_PASSWORD,\n",
        ")\n",
        "\n",
        "# Initialize the LLM from Groq\n",
        "llm = ChatGroq(\n",
        "    groq_api_key=GROQ_API_KEY,\n",
        "    model_name=\"llama-3.3-70b-versatile\",  # Updated to a supported model, llama-3.3-70b-versatile, groq/compound,\n",
        "    temperature=0.0,\n",
        ")\n",
        "\n",
        "graph, llm"
      ],
      "id": "tE54xbjQaiK4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHRHB5PDaiK4"
      },
      "source": [
        "## 5. Our realistic scenario: DevOps incident management\n",
        "\n",
        "Imagine we are an SRE / DevOps team.  \n",
        "We want to answer questions like:\n",
        "\n",
        "- *\"Which services had P1 incidents?\"*  \n",
        "- *\"Who handled incident INC-003?\"*  \n",
        "- *\"Which engineer has handled the most incidents?\"*  \n",
        "\n",
        "This is a good fit for a **knowledge graph** with entities and relationships like:\n",
        "\n",
        "- `(:Service {name})`\n",
        "- `(:Incident {id, severity, started_at, ended_at})`\n",
        "- `(:Engineer {name})`\n",
        "- `(:Service)-[:HAS_INCIDENT]->(:Incident)`\n",
        "- `(:Incident)-[:HANDLED_BY]->(:Engineer)`"
      ],
      "id": "XHRHB5PDaiK4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVaTVPXSaiK5"
      },
      "source": [
        "## 6. Create a small incident knowledge graph in Neo4j\n",
        "\n",
        "We now create a tiny dataset directly in Neo4j using a Cypher query.\n",
        "\n",
        "This query will:\n",
        "\n",
        "- Create a few **services** (e.g. `checkout-service`, `payment-service`, `user-service`)\n",
        "- Create some **incidents** (with `id`, `severity`, and timestamps)\n",
        "- Create **engineers** (e.g. `Alice`, `Bob`, `Carlos`)\n",
        "- Connect them with relationships:\n",
        "  - `(:Service)-[:HAS_INCIDENT]->(:Incident)`\n",
        "  - `(:Incident)-[:HANDLED_BY]->(:Engineer)`\n",
        "\n",
        "We use `MERGE` instead of `CREATE` to avoid duplicates if you run the cell multiple times."
      ],
      "id": "ZVaTVPXSaiK5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fimgj0FSaiK5",
        "outputId": "8b228b90-f179-4535-82c4-371f634127eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "cleanup_query_1 = \"\"\"\n",
        "MATCH (n:Service)-[r1:HAS_INCIDENT]->(i:Incident)\n",
        "DETACH DELETE n, i\n",
        "\"\"\"\n",
        "\n",
        "cleanup_query_2 = \"\"\"\n",
        "MATCH (e:Engineer)\n",
        "DETACH DELETE e\n",
        "\"\"\"\n",
        "\n",
        "data_creation_query = \"\"\"\n",
        "// Create services\n",
        "MERGE (s1:Service {name: \"checkout-service\"})\n",
        "MERGE (s2:Service {name: \"payment-service\"})\n",
        "MERGE (s3:Service {name: \"user-service\"})\n",
        "\n",
        "// Create engineers\n",
        "MERGE (e1:Engineer {name: \"Alice\"})\n",
        "MERGE (e2:Engineer {name: \"Bob\"})\n",
        "MERGE (e3:Engineer {name: \"Carlos\"})\n",
        "\n",
        "// Create incidents\n",
        "MERGE (i1:Incident {\n",
        "  id: \"INC-001\",\n",
        "  severity: \"P1\",\n",
        "  summary: \"Checkout failures for EU customers\",\n",
        "  started_at: datetime(\"2024-08-01T09:15:00Z\"),\n",
        "  ended_at: datetime(\"2024-08-01T09:45:00Z\")\n",
        "})\n",
        "MERGE (i2:Incident {\n",
        "  id: \"INC-002\",\n",
        "  severity: \"P2\",\n",
        "  summary: \"Intermittent payment timeouts\",\n",
        "  started_at: datetime(\"2024-08-05T11:00:00Z\"),\n",
        "  ended_at: datetime(\"2024-08-05T12:30:00Z\")\n",
        "})\n",
        "MERGE (i3:Incident {\n",
        "  id: \"INC-003\",\n",
        "  severity: \"P1\",\n",
        "  summary: \"User profile service returning 500 errors\",\n",
        "  started_at: datetime(\"2024-08-10T15:20:00Z\"),\n",
        "  ended_at: datetime(\"2024-08-10T16:05:00Z\")\n",
        "})\n",
        "\n",
        "// Connect services to incidents\n",
        "MERGE (s1)-[:HAS_INCIDENT]->(i1)\n",
        "MERGE (s2)-[:HAS_INCIDENT]->(i2)\n",
        "MERGE (s3)-[:HAS_INCIDENT]->(i3)\n",
        "\n",
        "// Connect incidents to engineers who handled them\n",
        "MERGE (i1)-[:HANDLED_BY]->(e1)\n",
        "MERGE (i2)-[:HANDLED_BY]->(e2)\n",
        "MERGE (i3)-[:HANDLED_BY]->(e1)\n",
        "\"\"\"\n",
        "\n",
        "# Run the Cypher queries against Neo4j\n",
        "graph.query(cleanup_query_1)\n",
        "graph.query(cleanup_query_2)\n",
        "graph.query(data_creation_query)"
      ],
      "id": "Fimgj0FSaiK5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7YrA04RaiK6"
      },
      "source": [
        "## 7. Inspect the graph schema\n",
        "\n",
        "Now we ask Neo4j (through LangChain's `Neo4jGraph`) to:\n",
        "\n",
        "1. Refresh the schema (scan labels, relationships, and properties).\n",
        "2. Print the schema so we can see what the graph looks like.\n",
        "\n",
        "This schema will later help the LLM generate better Cypher queries."
      ],
      "id": "y7YrA04RaiK6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7qOKRDyaiK7",
        "outputId": "2370ba8c-5756-47c5-c64c-52853535a49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node properties:\n",
            "Service {name: STRING}\n",
            "Engineer {name: STRING}\n",
            "Incident {severity: STRING, summary: STRING, started_at: DATE_TIME, id: STRING, ended_at: DATE_TIME}\n",
            "Relationship properties:\n",
            "\n",
            "The relationships:\n",
            "(:Service)-[:HAS_INCIDENT]->(:Incident)\n",
            "(:Incident)-[:HANDLED_BY]->(:Engineer)\n"
          ]
        }
      ],
      "source": [
        "graph.refresh_schema()\n",
        "print(graph.schema)"
      ],
      "id": "D7qOKRDyaiK7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qatZR2f_aiK7"
      },
      "source": [
        "## 8. From unstructured incident text to a graph (Graph RAG idea)\n",
        "\n",
        "In many real systems, our incident knowledge does not come as perfect rows.  \n",
        "We usually have **incident reports**, **postmortems**, and **Slack messages**.\n",
        "\n",
        "Here we:\n",
        "\n",
        "1. Create a short **incident report** as plain text.\n",
        "2. Wrap it in a LangChain `Document`.\n",
        "3. Use `LLMGraphTransformer` to **extract entities and relationships** from the text.\n",
        "\n",
        "This gives us a **graph view** of an unstructured document, which is the essence of **Graph RAG**."
      ],
      "id": "qatZR2f_aiK7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO3jYsPRaiK8",
        "outputId": "61b79c51-d213-4736-9901-c97670d30b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[GraphDocument(nodes=[Node(id='P1 Incident', type='Incident', properties={}), Node(id='User-Service', type='Service', properties={}), Node(id='Http 500 Errors', type='Error', properties={}), Node(id='Carlos', type='Person', properties={}), Node(id='Alice', type='Person', properties={}), Node(id='Faulty Deployment', type='Deployment', properties={}), Node(id='Feature Flag', type='Feature', properties={})], relationships=[Relationship(source=Node(id='User-Service', type='Service', properties={}), target=Node(id='P1 Incident', type='Incident', properties={}), type='EXPERIENCED', properties={}), Relationship(source=Node(id='Users', type='Person', properties={}), target=Node(id='Http 500 Errors', type='Error', properties={}), type='RECEIVED', properties={}), Relationship(source=Node(id='Carlos', type='Person', properties={}), target=Node(id='On-Call', type='Role', properties={}), type='INVESTIGATED', properties={}), Relationship(source=Node(id='Alice', type='Person', properties={}), target=Node(id='Faulty Deployment', type='Deployment', properties={}), type='ROLLED_BACK', properties={}), Relationship(source=Node(id='Feature Flag', type='Feature', properties={}), target=Node(id='User-Service', type='Service', properties={}), type='AFFECTED', properties={})], source=Document(metadata={}, page_content='On August 10th, 2024, the user-service experienced a P1 incident.\\nUsers were receiving HTTP 500 errors when viewing their profile page.\\nThe on-call engineer Carlos investigated but later handed over to Alice,\\nwho rolled back a faulty deployment and restored the service.\\nThe root cause was a misconfigured feature flag affecting user-service reads.\\n'))]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "\n",
        "incident_report_text = \"\"\"On August 10th, 2024, the user-service experienced a P1 incident.\n",
        "Users were receiving HTTP 500 errors when viewing their profile page.\n",
        "The on-call engineer Carlos investigated but later handed over to Alice,\n",
        "who rolled back a faulty deployment and restored the service.\n",
        "The root cause was a misconfigured feature flag affecting user-service reads.\n",
        "\"\"\"\n",
        "\n",
        "documents = [Document(page_content=incident_report_text)]\n",
        "\n",
        "# Use the LLM to transform text into graph-structured data\n",
        "graph_transformer = LLMGraphTransformer(llm=llm)\n",
        "graph_documents = graph_transformer.convert_to_graph_documents(documents)\n",
        "\n",
        "graph_documents"
      ],
      "id": "JO3jYsPRaiK8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dKYarOiaiK8"
      },
      "source": [
        "## 9. Inspect extracted nodes and relationships\n",
        "\n",
        "`graph_documents` contains the LLM-extracted graph representation of the incident report.\n",
        "\n",
        "We will:\n",
        "\n",
        "- Look at the **nodes** (entities)\n",
        "- Look at the **relationships** between those entities\n",
        "\n",
        "This step helps you visually understand what the LLM \"saw\" in the text."
      ],
      "id": "2dKYarOiaiK8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpZfpBybaiK9",
        "outputId": "8d89df4d-ca49-49b3-e60f-b7d0ea7a369c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Nodes ===\n",
            "id='P1 Incident' type='Incident' properties={}\n",
            "id='User-Service' type='Service' properties={}\n",
            "id='Http 500 Errors' type='Error' properties={}\n",
            "id='Carlos' type='Person' properties={}\n",
            "id='Alice' type='Person' properties={}\n",
            "id='Faulty Deployment' type='Deployment' properties={}\n",
            "id='Feature Flag' type='Feature' properties={}\n",
            "\n",
            "=== Relationships ===\n",
            "source=Node(id='User-Service', type='Service', properties={}) target=Node(id='P1 Incident', type='Incident', properties={}) type='EXPERIENCED' properties={}\n",
            "source=Node(id='Users', type='Person', properties={}) target=Node(id='Http 500 Errors', type='Error', properties={}) type='RECEIVED' properties={}\n",
            "source=Node(id='Carlos', type='Person', properties={}) target=Node(id='On-Call', type='Role', properties={}) type='INVESTIGATED' properties={}\n",
            "source=Node(id='Alice', type='Person', properties={}) target=Node(id='Faulty Deployment', type='Deployment', properties={}) type='ROLLED_BACK' properties={}\n",
            "source=Node(id='Feature Flag', type='Feature', properties={}) target=Node(id='User-Service', type='Service', properties={}) type='AFFECTED' properties={}\n"
          ]
        }
      ],
      "source": [
        "first_graph_doc = graph_documents[0]\n",
        "\n",
        "print(\"=== Nodes ===\")\n",
        "for node in first_graph_doc.nodes:\n",
        "    print(node)\n",
        "\n",
        "print(\"\\n=== Relationships ===\")\n",
        "for rel in first_graph_doc.relationships:\n",
        "    print(rel)"
      ],
      "id": "OpZfpBybaiK9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB5MW91RaiK9"
      },
      "source": [
        "> Note: In a real production system, the next step would be to **persist these extracted nodes and relationships into Neo4j**.\n",
        ">\n",
        "> For this beginner-friendly demo we focus on:\n",
        "> - Understanding the extraction step\n",
        "> - Asking questions against the **structured incident graph** we manually inserted earlier.\n",
        ">\n",
        "> Combining both steps (auto-ingesting extracted graphs into Neo4j) is a natural next improvement."
      ],
      "id": "mB5MW91RaiK9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntcI9L8faiK-"
      },
      "source": [
        "## 10. Build a GraphCypherQAChain (NL → Cypher → Neo4j → Answer)\n",
        "\n",
        "Now we build a `GraphCypherQAChain`:\n",
        "\n",
        "- Input: **natural language question** (`\"Which services had P1 incidents?\"`)\n",
        "- Internal steps:\n",
        "  1. LLM looks at the **graph schema**.\n",
        "  2. LLM generates a **Cypher query**.\n",
        "  3. The query runs against Neo4j.\n",
        "  4. LLM turns the raw result rows into a **friendly natural language answer**.\n",
        "\n",
        "We set `verbose=True` so we can see the generated Cypher for learning purposes."
      ],
      "id": "ntcI9L8faiK-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd9dNKZ0aiK_",
        "outputId": "264ded4f-2118-4ffd-ff09-a25ad7e922c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GraphCypherQAChain(verbose=True, graph=<langchain_community.graphs.neo4j_graph.Neo4jGraph object at 0x7e26355f2150>, cypher_generation_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question', 'schema'], input_types={}, partial_variables={}, template='Task:Generate Cypher statement to query a graph database.\\nInstructions:\\nUse only the provided relationship types and properties in the schema.\\nDo not use any other relationship types or properties that are not provided.\\nSchema:\\n{schema}\\nNote: Do not include any explanations or apologies in your responses.\\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\nDo not include any text except the generated Cypher statement.\\n\\nThe question is:\\n{question}'), llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7e263547ba10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7e262fe1f020>, model_name='llama-3.3-70b-versatile', temperature=1e-08, model_kwargs={}, groq_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), qa_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant that turns Neo4j query results into natural language answers.\\n\\nREAD THIS CAREFULLY:\\nThe `context` shown below is the EXACT and COMPLETE output of the Cypher query\\nthat already answers the question. You must ONLY base your answer on these rows.\\n\\ncontext:\\n{context}\\n\\nquestion:\\n{question}\\n\\nRULES (follow exactly):\\n1. If `context` is an empty list `[]`, answer exactly:\\n   I don't know based on the data provided.\\n2. If `context` is NOT empty, you MUST answer using the values inside it.\\n3. NEVER say the data is missing, incomplete, or does not mention something.\\n4. NEVER claim you don't know when `context` is NOT empty.\\n5. NEVER reinterpret or doubt the meaning of the context. It is ALWAYS correct.\\n6. Provide a CLEAR and direct answer based solely on the rows.\\n\\nNow provide the final answer:\"), llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7e263547ba10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7e262fe1f020>, model_name='llama-3.3-70b-versatile', temperature=1e-08, model_kwargs={}, groq_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), graph_schema='Node properties are the following:\\nService {name: STRING},Engineer {name: STRING},Incident {severity: STRING, summary: STRING, started_at: DATE_TIME, id: STRING, ended_at: DATE_TIME}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Service)-[:HAS_INCIDENT]->(:Incident),(:Incident)-[:HANDLED_BY]->(:Engineer)', allow_dangerous_requests=True)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain\n",
        "\n",
        "qa_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=(\n",
        "        \"You are an assistant that turns Neo4j query results into natural language answers.\\n\\n\"\n",
        "        \"READ THIS CAREFULLY:\\n\"\n",
        "        \"The `context` shown below is the EXACT and COMPLETE output of the Cypher query\\n\"\n",
        "        \"that already answers the question. You must ONLY base your answer on these rows.\\n\\n\"\n",
        "        \"context:\\n{context}\\n\\n\"\n",
        "        \"question:\\n{question}\\n\\n\"\n",
        "        \"RULES (follow exactly):\\n\"\n",
        "        \"1. If `context` is an empty list `[]`, answer exactly:\\n\"\n",
        "        \"   I don't know based on the data provided.\\n\"\n",
        "        \"2. If `context` is NOT empty, you MUST answer using the values inside it.\\n\"\n",
        "        \"3. NEVER say the data is missing, incomplete, or does not mention something.\\n\"\n",
        "        \"4. NEVER claim you don't know when `context` is NOT empty.\\n\"\n",
        "        \"5. NEVER reinterpret or doubt the meaning of the context. It is ALWAYS correct.\\n\"\n",
        "        \"6. Provide a CLEAR and direct answer based solely on the rows.\\n\\n\"\n",
        "        \"Now provide the final answer:\"\n",
        "\n",
        "    ),\n",
        ")\n",
        "\n",
        "qa_chain = GraphCypherQAChain.from_llm(\n",
        "    llm=llm,\n",
        "    graph=graph,\n",
        "    verbose=True,\n",
        "    qa_prompt=qa_prompt,\n",
        "    allow_dangerous_requests=True,\n",
        ")\n",
        "\n",
        "qa_chain"
      ],
      "id": "dd9dNKZ0aiK_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZqIQZsWaiLA"
      },
      "source": [
        "## 11. Ask natural language questions about incidents\n",
        "\n",
        "Now we are ready to ask questions in plain English and let the LLM:\n",
        "\n",
        "1. Translate them into Cypher\n",
        "2. Query Neo4j\n",
        "3. Summarize the results\n",
        "\n",
        "We will test a few examples."
      ],
      "id": "7ZqIQZsWaiLA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bs1PVOgaiLA",
        "outputId": "d392a61c-e714-4119-cd81-3c96a0a9764a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mMATCH (s:Service)-[:HAS_INCIDENT]->(i:Incident) \n",
            "WHERE i.severity = 'P2' \n",
            "RETURN DISTINCT s.name\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'s.name': 'payment-service'}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Which services had P2 incidents?',\n",
              " 'result': 'The payment-service had P2 incidents.'}"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "# 11.1 Which services had P1 incidents?\n",
        "response_1 = qa_chain.invoke({\n",
        "    \"query\": \"Which services had P2 incidents?\"\n",
        "})\n",
        "response_1"
      ],
      "id": "2bs1PVOgaiLA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mnt-omk6aiLB",
        "outputId": "d7201799-8d0d-4263-ed38-6194dbf92dbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mMATCH (i:Incident {id: 'INC-003'})-[:HANDLED_BY]->(e:Engineer) RETURN e.name\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'e.name': 'Alice'}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Who handled incident INC-003?',\n",
              " 'result': 'Alice handled incident INC-003.'}"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "# 11.2 Who handled incident INC-003?\n",
        "response_2 = qa_chain.invoke({\n",
        "    \"query\": \"Who handled incident INC-003?\"\n",
        "})\n",
        "response_2"
      ],
      "id": "Mnt-omk6aiLB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyBuFvd6aiLC",
        "outputId": "bc92242c-6919-43c2-e90d-6972d0bd3cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mMATCH (e:Engineer {name: 'Alice'})-[:HANDLED_BY]->(i:Incident) RETURN i.severity, i.id\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'List all incidents handled by Alice with their severity.',\n",
              " 'result': \"I don't know based on the data provided.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "# 11.3 List all incidents handled by Alice with their severity.\n",
        "response_3 = qa_chain.invoke({\n",
        "    \"query\": \"List all incidents handled by Alice with their severity.\"\n",
        "})\n",
        "response_3"
      ],
      "id": "EyBuFvd6aiLC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sCPgsz6aiLC",
        "outputId": "d9f12c5d-9bc1-4049-8160-fcc65aaa4081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mcypher\n",
            "MATCH (s:Service)-[:HAS_INCIDENT]->(i:Incident)-[:HANDLED_BY]->(e:Engineer)\n",
            "RETURN s.name AS Service, collect(i.id) AS Incidents, collect(e.name) AS Engineers\n",
            "\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'Service': 'checkout-service', 'Incidents': ['INC-001'], 'Engineers': ['Alice']}, {'Service': 'payment-service', 'Incidents': ['INC-002'], 'Engineers': ['Bob']}, {'Service': 'user-service', 'Incidents': ['INC-003'], 'Engineers': ['Alice']}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'For each service, list its incidents and the engineers who handled them.',\n",
              " 'result': 'The checkout-service had incidents INC-001, which were handled by Alice. \\nThe payment-service had incidents INC-002, which were handled by Bob. \\nThe user-service had incidents INC-003, which were handled by Alice.'}"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "# 11.4 For each service, list its incidents and the engineers who handled them.\n",
        "response_4 = qa_chain.invoke({\n",
        "    \"query\": \"For each service, list its incidents and the engineers who handled them.\"\n",
        "})\n",
        "response_4"
      ],
      "id": "4sCPgsz6aiLC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WufhVYPXaiLC"
      },
      "source": [
        "## 12. (Optional) Manual Cypher examples\n",
        "\n",
        "Sometimes you want to write Cypher by hand to debug or explore the data.\n",
        "\n",
        "Below are some manual queries you can run.  \n",
        "These do not use the LLM; they are pure Neo4j Cypher."
      ],
      "id": "WufhVYPXaiLC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88W7HjZXaiLD"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# All services and their incidents\n",
        "graph.query(\"\"\"\n",
        "MATCH (s:Service)-[:HAS_INCIDENT]->(i:Incident)\n",
        "RETURN s.name AS service, i.id AS incident_id, i.severity AS severity, i.summary AS summary\n",
        "ORDER BY service, incident_id\n",
        "\"\"\")"
      ],
      "id": "88W7HjZXaiLD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hzZo4P9aiLD"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Count incidents per engineer\n",
        "graph.query(\"\"\"\n",
        "MATCH (i:Incident)-[:HANDLED_BY]->(e:Engineer)\n",
        "RETURN e.name AS engineer, count(i) AS incident_count\n",
        "ORDER BY incident_count DESC\n",
        "\"\"\")"
      ],
      "id": "6hzZo4P9aiLD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro8UfFdAaiLD"
      },
      "source": [
        "## 13. Summary and next steps\n",
        "\n",
        "In this notebook you:\n",
        "\n",
        "1. Connected Colab to **Neo4j Aura**.\n",
        "2. Created a small **DevOps incident knowledge graph**.\n",
        "3. Used an LLM to **extract a graph** from an unstructured incident report (Graph RAG idea).\n",
        "4. Built a **GraphCypherQAChain** to:\n",
        "   - Convert natural language questions into Cypher.\n",
        "   - Query Neo4j.\n",
        "   - Generate friendly answers.\n",
        "\n",
        "**Ideas to extend this demo:**\n",
        "\n",
        "- Ingest **real incident postmortems** from your organization.\n",
        "- Automatically **persist** extracted nodes and relationships into Neo4j.\n",
        "- Combine **vector search (embeddings)** and **graph queries** for hybrid RAG.\n",
        "- Add more entities: teams, runbooks, on-call rotations, services in multiple regions, etc."
      ],
      "id": "Ro8UfFdAaiLD"
    }
  ]
}